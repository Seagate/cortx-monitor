#!/bin/bash

# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify it under the
# terms of the GNU Affero General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License along
# with this program. If not, see <https://www.gnu.org/licenses/>. For any questions
# about this software or licensing, please email opensource@seagate.com or
# cortx-questions@seagate.com.

# Stop the script if any command fails
set -e -u -o pipefail

script_dir=$(dirname $0)

# Import common constants
source $script_dir/constants.sh

SCRIPT_NAME=$(basename $0)

RSYSLOG_CONF="/etc/rsyslog.d/0-iemfwd.conf"
RSYSLOG_SSPL_CONF="/etc/rsyslog.d/1-ssplfwd.conf"
PACEMAKER_INSTALLATION_PATH="/lib/ocf/resource.d/seagate/"

usage() {
    cat << EOF
$SCRIPT_NAME [[-p <LDR_R1|LDR_R2>] [-e <DEV|PROD>] [-c|--rmq-cluster <true|false>]]
    -p Product to be configured
    -e Environment
    -c Need rmq cluster? (true or false)
EOF
    exit 1
}

RMQ_CLUSTER=true

while getopts ":p:e:c:" OPTION; do
    case $OPTION in
        p )
            PRODUCT=$OPTARG
            ;;
        e )
            ENVIRONMENT=$OPTARG
            ;;
        c )
            RMQ_CLUSTER=$OPTARG
            ;;
        * )
            usage
            ;;
    esac
done

# sspl_setup_consul script install consul in dev env and checks if consul process is running 
# on prod. For node replacement scenario consul will not be running on the new node. But,
# there will be two instance of consul running on healthy node. When new node is configured
# consul will be brought back on it. We are using VIP to connect to consul. So, if consul
# is not running on new node, we dont need to error out. So, need to skip this step for 
# node replacement case
# setup consul if not running already
[ -f $REPLACEMENT_NODE_ENV_VAR_FILE ] ||
    $script_dir/sspl_setup_consul -e $ENVIRONMENT

# Install packages which are not available in YUM repo, from PIP
python3 -m pip install -r $SSPL_BASE_DIR/low-level/requirements.txt

# NOTE: By default the sspl default conf file will not be copied.
# The provisioner is supposed to copy the appropriate conf file based
# on product/env and start SSPL with it.
# TODO: Disable this default copy once the provisioners are ready.
[ -f $SSPL_CONF ] || cp $SSPL_BASE_DIR/conf/sspl.conf.$PRODUCT $SSPL_CONF
# For Dev environment
[ "$ENVIRONMENT" == "DEV" ] && cp $SSPL_BASE_DIR/conf/sspl.conf.$PRODUCT $SSPL_CONF

# Copy rsyslog configuration
[ -f $RSYSLOG_CONF ] ||
    cp $SSPL_BASE_DIR/low-level/files/$RSYSLOG_CONF $RSYSLOG_CONF

[ -f $RSYSLOG_SSPL_CONF ] ||
    cp $SSPL_BASE_DIR/low-level/files/$RSYSLOG_SSPL_CONF $RSYSLOG_SSPL_CONF

# Create soft link for SINGLE product name service to existing LDR_R1 service
# Instead of keeping separate service file for SINGLE product with same content.
currentProduct=$SSPL_BASE_DIR/conf/sspl-ll.service.$PRODUCT
echo "currentProduct name $currentProduct"
if [ $PRODUCT == 'SINGLE' -a ! -f $currentProduct ] || [ $PRODUCT == 'DUAL' -a ! -f $currentProduct ]; then
    ln -s $SSPL_BASE_DIR/conf/sspl-ll.service.LDR_R1 $SSPL_BASE_DIR/conf/sspl-ll.service.$PRODUCT
fi
if [ $PRODUCT == 'CLUSTER' -a ! -f $currentProduct ]; then
  ln -s $SSPL_BASE_DIR/conf/sspl-ll.service.LDR_R2 $SSPL_BASE_DIR/conf/sspl-ll.service.$PRODUCT
fi
# Copy sspl-ll.service file and enable service
cp $SSPL_BASE_DIR/conf/sspl-ll.service.$PRODUCT /etc/systemd/system/sspl-ll.service
systemctl enable sspl-ll.service
systemctl daemon-reload

# Copy IEC mapping files
mkdir -p $PRODUCT_BASE_DIR/iem/iec_mapping
cp -R $SSPL_BASE_DIR/low-level/files/iec_mapping/* $PRODUCT_BASE_DIR/iem/iec_mapping

# Skip this step if sspl is being configured for node replacement scenario as sspl configurations are
# already available in consul on the healthy node
# Running script for fetching the config using salt and feeding values to consul
[ -f $REPLACEMENT_NODE_ENV_VAR_FILE ] ||
    $script_dir/sspl_fetch_config_from_salt.py $ENVIRONMENT $PRODUCT

# Skip this step if sspl is being configured for node replacement scenario as rabbitmq cluster is
# already configured on the healthy node
# Configure rabbitmq
[ -f $REPLACEMENT_NODE_ENV_VAR_FILE ] || {
    [ "$RMQ_CLUSTER" == true ] && $script_dir/rabbitmq_setup
}
